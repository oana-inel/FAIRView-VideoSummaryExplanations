{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# modify the line below and add the path to your google credentials json file\n",
    "# further information on how to generate such a json file: https://cloud.google.com/docs/authentication/getting-started#command-line\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = PLACEHOLDER_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# in this notebook we need two Google Cloud Storage buckets: https://cloud.google.com/storage/docs/creating-buckets\n",
    "# in the first bucket we have the video subtitles that we extracted from the videos\n",
    "# in the second bucket we store the named entities extracted from the video subtitles\n",
    "\n",
    "# Replace all PLACEHOLDER_ variables with the variables you have created.\n",
    "def list_blobs_with_prefix(bucket_name, prefix, delimiter):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs(prefix=prefix, delimiter=delimiter)\n",
    "\n",
    "    videos = []\n",
    "    for blob in blobs:\n",
    "        if not blob.name.endswith(\"/\"):\n",
    "            videos.append(\"gs://\" + bucket_name + \"/\" + blob.name)\n",
    "            \n",
    "    return videos\n",
    "            \n",
    "videos = list_blobs_with_prefix(PLACEHOLDER_BUCKET_NAME_VIDEO_SUBTITLES, PLACEHOLDER_PREFIX, PLACEHOLDER_DELIMITER)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.cloud import storage\n",
    "\n",
    "import sys\n",
    "import six\n",
    "\n",
    "# Replace all PLACEHOLDER_ variables with the variables you have created.\n",
    "storage_client = storage.Client()\n",
    "\n",
    "gcloud_bucket_name_read = PLACEHOLDER_BUCKET_NAME_VIDEO_SUBTITLES\n",
    "bucket_read = storage_client.get_bucket(gcloud_bucket_name_read)\n",
    "\n",
    "gcloud_bucket_name_write = PLACEHOLDER_BUCKET_NAME_VIDEO_SUBTITLES_ENTITIES\n",
    "bucket_write = storage_client.get_bucket(gcloud_bucket_name_write)\n",
    "\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "names = [\"storageLink\",\"entity\",\"entity_type\",\"mid\",\"wikipedia_url\",\"entity_mention\",\"begin_offset\",\"mention_sentiment_magnitude\",\"mention_sentiment_score\", \"mention_type\", \"entity_salience\", \"entity_sentiment_magnitude\", \"entity_sentiment_score\"]\n",
    "\n",
    "\n",
    "for video in videos:\n",
    "\n",
    "    videoComp = video.split(\"/\")\n",
    "    \n",
    "    with open(\"../data/named_entities_subtitles/\" + videoComp[-1], \"w\" ) as outSentences:\n",
    "        writer = csv.writer( outSentences )\n",
    "        writer.writerow(names)\n",
    "\n",
    "    # PLACEHOLDER_WORKING_FILE refers to local file that is only used for processing purposes\n",
    "    blob = bucket_read.blob(videoComp[-1])\n",
    "    blob.download_to_filename(PLACEHOLDER_WORKING_FILE)\n",
    "\n",
    "    file = pd.read_csv(PLACEHOLDER_WORKING_FILE)\n",
    "\n",
    "    df = file.groupby(by=['storageLink','transcript','transcript_part'], as_index=False).first()\n",
    "    df = df[[\"storageLink\", \"transcript\", \"transcript_part\", \"start_time\", \"end_time\"]]\n",
    "    df = df.sort_values([\"transcript_part\"])\n",
    "\n",
    "    text = \" \".join(list(df[\"transcript\"]))\n",
    "\n",
    "    \n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    document = types.Document(content=text.encode('utf-8'), type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detect and send native Python encoding to receive correct word offsets.\n",
    "    encoding = enums.EncodingType.UTF32\n",
    "    if sys.maxunicode == 65535:\n",
    "        encoding = enums.EncodingType.UTF16\n",
    "\n",
    "    result = client.analyze_entity_sentiment(document, encoding)\n",
    "\n",
    "    for entity in result.entities:\n",
    "        for mention in entity.mentions:\n",
    "            line = [videoComp[-1][0:-8]]\n",
    "            entity_type = enums.Entity.Type(entity.type)\n",
    "            mention_type = enums.EntityMention.Type(mention.type)\n",
    "            line.extend([entity.name, entity_type.name, entity.metadata.get('mid', ''),entity.metadata.get('wikipedia_url', ''), mention.text.content, mention.text.begin_offset, mention.sentiment.magnitude, mention.sentiment.score, mention_type.name, entity.salience, entity.sentiment.magnitude, entity.sentiment.score])        \n",
    "\n",
    "            with open(\"../data/named_entities_subtitles/\" + videoComp[-1], \"a\" ) as outSentences:\n",
    "                writer = csv.writer( outSentences )\n",
    "                writer.writerow(line)\n",
    "    blob = bucket_write.blob(videoComp[-1])\n",
    "    blob.upload_from_filename(\"../data/named_entities_subtitles/\" + videoComp[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_keywords(keyword):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(str(keyword)))  \n",
    "    wn_tagged = map(lambda x: (str(x[0]), nltk2wn_tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:            \n",
    "            res_word = wordnet._morphy(str(word), wordnet.NOUN)\n",
    "            if res_word == []:\n",
    "                res_words.append(str(word))\n",
    "            else:\n",
    "                if len(res_word) == 1:\n",
    "                    res_words.append(str(res_word[0]))\n",
    "                else:\n",
    "                    res_words.append(str(res_word[1]))\n",
    "        else:\n",
    "            res_word = wordnet._morphy(str(word), tag)\n",
    "            if res_word == []:\n",
    "                res_words.append(str(word))\n",
    "            else: \n",
    "                if len(res_word) == 1:\n",
    "                    res_words.append(str(res_word[0]))\n",
    "                else:\n",
    "                    res_words.append(str(res_word[1]))\n",
    "        \n",
    "    lematized_keyword = \" \".join(res_words)\n",
    "  \n",
    "    return lematized_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of the notebook is used to enrich the type of named entities by looking into various knowledge bases, such as:\n",
    "#  - Wikidata type\n",
    "#  - DBpedia type\n",
    "#  - WordNet type\n",
    "\n",
    "# we also update the files with the lemma of each named entity and the lowercase lemma of each named entity\n",
    "\n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.cloud import storage\n",
    "\n",
    "import sys\n",
    "import six\n",
    "\n",
    "\n",
    "\"\"\"Example of Python client calling Knowledge Graph Search API.\"\"\"\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "# extract the wikipedia page linked to the dbpedia page\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "\n",
    "api_key = PLACEHOLDER_API_KEY_KGSEARCH\n",
    "service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "storage_client = storage.Client()\n",
    "\n",
    "gcloud_bucket_name_read = PLACEHOLDER_BUCKET_NAME_VIDEO_SUBTITLES_ENTITIES\n",
    "bucket_read = storage_client.get_bucket(gcloud_bucket_name_read)\n",
    "\n",
    "gcloud_bucket_name_write = PLACEHOLDER_BUCKET_NAME_VIDEO_SUBTITLES_ENTITIES\n",
    "bucket_write = storage_client.get_bucket(gcloud_bucket_name_write)\n",
    "\n",
    "video_entities = list_blobs_with_prefix(PLACEHOLDER_BUCKET_NAME_VIDEo_SUBTITLES_ENTITIES, PLACEHOLDER_PREFIX, PLACEHOLDER_DELIMITER)\n",
    "\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "person = [\"noun.person\"]\n",
    "organization = [\"noun.group\"]\n",
    "location = [\"noun.location\"]\n",
    "event = [\"noun.event\"]\n",
    "\n",
    "for video in video_entities:\n",
    "    \n",
    "    videoComp = video.split(\"/\")\n",
    "\n",
    "    blob = bucket_read.blob(videoComp[-1])\n",
    "    blob.download_to_filename(PLACEHOLDER_WORKING_FILE)\n",
    "\n",
    "    file = pd.read_csv(PLACEHOLDER_WORKING_FILE)\n",
    "    \n",
    "    file[\"entity_lemma\"] = \"\"\n",
    "    file[\"entity_lemma_lower\"] = \"\"\n",
    "    file[\"dbpedia_type\"] = \"\"\n",
    "    file[\"wikidata_type\"] = \"\"\n",
    "    file[\"all_dbpedia_types\"] = \"\"\n",
    "    file[\"all_wikidata_types\"] = \"\"\n",
    "    file[\"WordNet Type\"] = \"\"\n",
    "\n",
    "    for i in range(len(file)):\n",
    "        file[\"entity_lemma\"].iloc[i] = lemmatize_keywords(file[\"entity\"].iloc[i])\n",
    "        file[\"entity_lemma_lower\"].iloc[i] = lemmatize_keywords(file[\"entity\"].iloc[i].lower())\n",
    "\n",
    "        query = file[\"entity\"].iloc[i]\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'limit': 5,\n",
    "            'indent': True,\n",
    "            'key': api_key,\n",
    "        }\n",
    "        url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "        response = json.loads(urllib.request.urlopen(url).read())\n",
    "        found = False\n",
    "        types = []\n",
    "        all_types = []\n",
    "        for element in response['itemListElement']:\n",
    "            #print(element)\n",
    "            if 'name' in element['result']:\n",
    "                if element['result']['name'].lower() == query.lower() or element['result']['name'].lower() == file[\"entity\"].iloc[i].lower():\n",
    "                    all_types.extend(element['result']['@type'])\n",
    "                    if 'Event' in element['result']['@type']:\n",
    "                        types.append(\"EVENT\")\n",
    "                        found = True\n",
    "                    if 'Organization' in element['result']['@type']:\n",
    "                        types.append(\"ORGANIZATION\")\n",
    "                        found = True\n",
    "                    if 'Place' in element['result']['@type']:\n",
    "                        types.append(\"LOCATION\")\n",
    "                        found = True\n",
    "                    if 'Person' in element['result']['@type']:\n",
    "                        types.append(\"PERSON\")\n",
    "                        found = True\n",
    "\n",
    "        if found == False:\n",
    "            types.append(\"OTHER\")\n",
    "        else:\n",
    "            types = list(set(types))\n",
    "\n",
    "        file[\"wikidata_type\"].iloc[i] = \", \".join(types)\n",
    "\n",
    "        if len(all_types) != 0:\n",
    "            file[\"all_wikidata_types\"].iloc[i] = \", \".join(list(set(all_types)))\n",
    "        else:\n",
    "            file[\"all_wikidata_types\"].iloc[i] = \"OTHER\"\n",
    "\n",
    "            \n",
    "\n",
    "        page = \"http://dbpedia.org/resource/\" + file[\"entity\"].iloc[i].capitalize().replace(\" \", \"_\")\n",
    "        encoding = 'utf-8'\n",
    "\n",
    "        sparql.setQuery(\"\"\"\n",
    "            PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "            PREFIX dbp: <http://dbpedia.org/resource/>\n",
    "            PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "\n",
    "            SELECT ?types where {\n",
    "                <\"\"\" + page + \"\"\"> rdf:type ?types .\n",
    "            }\n",
    "        \"\"\")\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        ent_types = []\n",
    "\n",
    "        if len(results[\"results\"][\"bindings\"]) != 0:\n",
    "            for ent_type in results[\"results\"][\"bindings\"]:\n",
    "                if 'ontology' in ent_type[\"types\"][\"value\"]:\n",
    "                    ent_types.append(ent_type[\"types\"][\"value\"])\n",
    "\n",
    "        file[\"all_dbpedia_types\"].iloc[i] = \", \".join(list(set(ent_types)))\n",
    "\n",
    "        found = False\n",
    "        types = []\n",
    "        for ent_type in ent_types:\n",
    "\n",
    "            if 'Event' in ent_type:\n",
    "                types.append(\"EVENT\")\n",
    "                found = True\n",
    "            if 'Organization' in ent_type or 'Organisation' in ent_type:\n",
    "                types.append(\"ORGANIZATION\")\n",
    "                found = True\n",
    "            if 'Place' in ent_type:\n",
    "                types.append(\"LOCATION\")\n",
    "                found = True\n",
    "            if 'Person' in ent_type:\n",
    "                types.append(\"PERSON\")\n",
    "                found = True\n",
    "\n",
    "        if found == False:\n",
    "            file[\"dbpedia_type\"].iloc[i] = \"OTHER\"\n",
    "        else:\n",
    "            file[\"dbpedia_type\"].iloc[i] = \", \".join(list(set(types)))\n",
    "\n",
    "\n",
    "        types1 = []\n",
    "\n",
    "        for synset in wordnet.synsets(file[\"entity\"].iloc[i].replace(\" \", \"_\")):\n",
    "            #print(synset.lexname())\n",
    "            if synset.lexname() in person:\n",
    "                if \"PERSON\" not in types1:\n",
    "                    types1.append(\"PERSON\")\n",
    "            if synset.lexname() in organization:\n",
    "                if \"ORGANIZATION\" not in types1:\n",
    "                    types1.append(\"ORGANIZATION\")\n",
    "            if synset.lexname() in location:\n",
    "                if \"LOCATION\" not in types1:\n",
    "                    types1.append(\"LOCATION\")\n",
    "            if synset.lexname() in event:\n",
    "                if \"EVENT\" not in types1:\n",
    "                    types1.append(\"EVENT\")\n",
    "        if len(types1) != 0:\n",
    "            file[\"WordNet Type\"].iloc[i] = \", \".join(types1)\n",
    "    \n",
    "    file.to_csv(PLACEHOLDER_WORKING_FILE, index=False)\n",
    "    \n",
    "    blob = bucket_write.blob(videoComp[-1])\n",
    "    blob.upload_from_filename(PLACEHOLDER_WORKING_FILE)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
